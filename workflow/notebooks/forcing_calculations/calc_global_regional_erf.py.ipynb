{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "\n",
    "from workflow.scripts.utils import regrid_global, calculate_pooled_variance\n",
    "import time\n",
    "import xarray as xr\n",
    "from functools import partial\n",
    "import xesmf\n",
    "import pandas as pd\n",
    "from pyclim_noresm.general_util_funcs import global_avg\n",
    "from intake_esm.derived import DerivedVariableRegistry\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import yaml\n",
    "from scipy.stats._resampling import _bootstrap_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_datatracker(var_id):\n",
    "    import yaml\n",
    "    find_issue = lambda d: [k for k,v in d.items() if var_id in v]\n",
    "    erfs = snakemake.config.get('variables')\n",
    "    burdens_dict = snakemake.config.get('burdens_dict')\n",
    "    burdens_vars = find_issue(burdens_dict)\n",
    "    erfs_vars = find_issue(erfs)\n",
    "    issue_vars = burdens_vars+erfs_vars\n",
    "    if not issue_vars:\n",
    "        issue_vars = [var_id]\n",
    "        \n",
    "    \n",
    "    with open(snakemake.input.data_tracker, 'r') as f:\n",
    "        data = yaml.load(f, yaml.SafeLoader)\n",
    " \n",
    "    for v in issue_vars:\n",
    "        for k,requests in data.items():\n",
    "            data[k] = list(set(requests)-set(issue_vars)) \n",
    "    \n",
    "    with open(snakemake.input.data_tracker, 'w') as f:\n",
    "        yaml.dump(data,f, yaml.SafeDumper,default_flow_style=False)\n",
    "    \n",
    "    raise ValueError(f\"Could not find {var_id} in, removing {','.join(issue_vars)} from data tracker \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464cb384",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_exps = {'piClim-2xdust':'piClim-control'} # Should put this in config at some point\n",
    "\n",
    "conf = snakemake.config\n",
    "\n",
    "if snakemake.rule == \"calc_dust_regional_erf_table\":\n",
    "    exp_id = 'piClim-2xdust'\n",
    "else:\n",
    "    exp_id = snakemake.wildcards.experiment\n",
    "mod_id = snakemake.wildcards.model\n",
    "ctrl_id = control_exps.get(exp_id, 'piClim-control')\n",
    "time_slice = conf.get('time_slice', slice(2,None))\n",
    "\n",
    "nSamples = snakemake.params.get('nSamples', 1000)\n",
    "confidence_level = snakemake.params.get('conf_level', 0.95)\n",
    "\n",
    "mask = snakemake.input.get('mask')\n",
    "\n",
    "if mask:\n",
    "    MASK = xr.open_dataset(mask)\n",
    "\n",
    "if snakemake.config['model_specific_variant'].get(exp_id, None):\n",
    "    memb_id = snakemake.config['model_specific_variant'][exp_id].get(mod_id, snakemake.config['variant_default'])\n",
    "else:\n",
    "    memb_id = snakemake.config['variant_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be89574",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvr = DerivedVariableRegistry()\n",
    "@dvr.register(variable='rlutaf', \n",
    "              query=dict(experiment_id=[exp_id,ctrl_id],\n",
    "                    source_id=mod_id,\n",
    "                     variable_id = ['rlutcs', 'rlut','rlutcsaf'],\n",
    "                     )\n",
    "             )\n",
    "def calc_rlutaf(ds):\n",
    "    ds['rlutaf'] = ds['rlut'] + (ds['rlutcsaf'] - ds['rlutcs'])\n",
    "    attrs = ds['rlut'].attrs.copy()\n",
    "    ds = ds.drop(['rlut','rlutcsaf','rlutcs'])\n",
    "    ds['rlutaf'].attrs = attrs \n",
    "    ds['rlutaf'].attrs['long_name'] = 'TOA Outgoing aerosol free Longwave Radiation'\n",
    "    ds.attrs['variable_id'] = 'rlutaf'\n",
    "    return ds\n",
    "    \n",
    "@dvr.register(variable='rsutaf', \n",
    "              query=dict(experiment_id=[exp_id,ctrl_id],\n",
    "                    source_id=mod_id,\n",
    "                     variable_id = ['rsutcs', 'rsut','rsutcsaf'],\n",
    "                     )\n",
    "             )\n",
    "def calc_rsutaf(ds):\n",
    "    ds['rsutaf'] = ds['rsut'] + (ds['rsutcsaf'] - ds['rsutcs'])\n",
    "    attrs = ds['rsut'].attrs.copy()\n",
    "    ds = ds.drop(['rsut','rsutcsaf','rsutcs'])\n",
    "    ds['rsutaf'].attrs = attrs \n",
    "    ds['rsutaf'].attrs['long_name'] = 'TOA Outgoing aerosol free Shortwave Radiation'\n",
    "    ds.attrs['variable_id'] = 'rsutaf'\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable(var_id, exp_id, cat):\n",
    "    \n",
    "    use_derived_vars_dict = conf['use_derived_vars'].get(var_id,False)\n",
    "    if isinstance(use_derived_vars_dict, dict):\n",
    "        use_derived_vars = use_derived_vars_dict.get('all', False)\n",
    "        use_derived_vars = use_derived_vars_dict.get(mod_id,use_derived_vars)\n",
    "    else:\n",
    "        use_derived_vars = use_derived_vars_dict\n",
    "    \n",
    "    if conf['model_specific_variant'].get(exp_id, None):\n",
    "        memb_id = conf['model_specific_variant'][exp_id].get(mod_id, conf['variant_default'])\n",
    "    else:\n",
    "        memb_id = conf['variant_default']\n",
    "        \n",
    "    col = cat.search(experiment_id=exp_id,\n",
    "                source_id=mod_id,\n",
    "                 variable_id = var_id,\n",
    "                 member_id=memb_id,\n",
    "    )\n",
    "\n",
    "    if var_id in col.unique()['variable_id'] and use_derived_vars==False:\n",
    "        col.derivedcat = DerivedVariableRegistry()\n",
    "        col = col.search(variable_id = var_id)\n",
    "        if col.nunique()['table_id'] > 1:\n",
    "            col = col.search(table_id=conf['table_ids'].get(var_id, conf['table_id_default']))\n",
    "    if col.nunique().version > 1 and not col.unique()['derived_variable_id']:\n",
    "        latest = max(col.df['version'].unique())\n",
    "        col = col.search(version=[latest])\n",
    "\n",
    "    try:\n",
    "        ds = col.to_dataset_dict(xarray_open_kwargs={'use_cftime':True},progressbar=False)\n",
    "        ds = ds[list(ds.keys())[0]]\n",
    "        ds = ds.drop('member_id').squeeze()\n",
    "    except IndexError:\n",
    "        update_datatracker(var_id)\n",
    "    if col.derivedcat.values():\n",
    "        expected_variables = col.derivedcat[var_id].query['variable_id']\n",
    "        if set(expected_variables) != set(col.unique()['variable_id']):\n",
    "            update_datatracker(var_id)\n",
    "    data = ds.copy()\n",
    "    data=data[data.variable_id].resample(time='Y').mean()\n",
    "    data = data.to_dataset(name=ds.variable_id)\n",
    "    data.attrs = ds.attrs.copy()\n",
    "    data.attrs['history'] = data.attrs.get('history','') + f', annual average'    \n",
    "    t=data.time.dt.strftime('%Y')\n",
    "    data = data.assign_coords({'time':xr.cftime_range(start=str(t[0].values), end= str(t[-1].values), freq='YS')})\n",
    "    if ds.cf.bounds.get('lon') is None:\n",
    "        data = data.cf.add_bounds(['lon'])\n",
    "    else:\n",
    "        data = data.assign({ds.cf.bounds['lon'][0]:ds[ds.cf.bounds['lon'][0]]})\n",
    "    if ds.cf.bounds.get('lat') is None:\n",
    "        data = data.cf.add_bounds(['lat'])\n",
    "    else:\n",
    "        data = data.assign({ds.cf.bounds['lat'][0]:ds[ds.cf.bounds['lat'][0]]})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce22d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid(ds, no_mask=False):\n",
    "    method='conservative'\n",
    "    ds = regrid_global(ds,MASK, method=method)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def calc_erf(variables, \n",
    "             exp_id, \n",
    "             cat, \n",
    "             time_slice=slice(2,None), \n",
    "             n_samples=1000,\n",
    "             mask=None,\n",
    "            only_regrid=False,\n",
    "            boot_strap=True):\n",
    "    dsets = {}\n",
    "    signs = {'rlut':-1, 'rlutcs' :-1, 'rlutaf': -1, 'rlutcsaf':-1 ,\n",
    "          'rsdt':1, \n",
    "          'rsut':-1, 'rsutcs': -1, 'rsutcsaf': -1, 'rsutaf':-1,\n",
    "          'rlus':-1,\n",
    "           'rsus':-1, 'rsuscs': -1, \n",
    "            'rsds':1, 'rsdscs': 1,\n",
    "            'rlds':1}\n",
    "    \n",
    "    try:\n",
    "        for var_id in variables:\n",
    "            dsets[var_id] = get_variable(var_id, exp_id, cat)\n",
    "    except:\n",
    "        raise ValueError(f\"Could not find {var_id}\")\n",
    "    data = {}\n",
    "    for k, ds in dsets.items():\n",
    "        try:\n",
    "            data[k] = ds.load()\n",
    "        except:\n",
    "            (f\"print couldn't not load {k}\")\n",
    "            raise\n",
    "    if only_regrid:\n",
    "        data = {k:regrid(ds) for k, ds in data.items()} \n",
    "    \n",
    "    elif mask is not None:\n",
    "        data = {k:regrid(ds).where(mask,0.0) for k, ds in data.items()} \n",
    "    \n",
    "    data = {k:global_avg(ds) for k, ds in data.items()}\n",
    "    forcing = 0\n",
    "    for v, ds in data.items():\n",
    "        if mod_id in ['NorESM2-LM','NorESM2.0.6dev-LM'] and v in ['rsutaf', 'rsutafcs']:\n",
    "            forcing += signs[v]*np.abs(ds[v]-data['rsdt']['rsdt'])\n",
    "        else:\n",
    "            forcing += signs[v]*ds[v]\n",
    "    forcing = forcing.isel(time=time_slice)\n",
    "    if boot_strap:\n",
    "        samples = _bootstrap_resample(forcing.values,n_samples)\n",
    "        mean, std = samples.mean(axis=-1).mean(), samples.mean(axis=-1).std()\n",
    "    else:\n",
    "        mean = forcing.mean()\n",
    "        std = forcing.std()\n",
    "        samples = forcings\n",
    "    return mean, std, forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212af371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_diff(exp_mean, exp_std, ctrl_mean, ctrl_std, n_samples=1000, confidence=0.95):\n",
    "    dst_ctrl = lambda :np.random.normal(ctrl_mean,ctrl_std)\n",
    "    dst_exp = lambda :np.random.normal(exp_mean,exp_std)\n",
    "    \n",
    "    diffs = [dst_exp()-dst_ctrl() for i in range(n_samples)]\n",
    "    diffs = np.array(diffs)\n",
    "    res = st.bootstrap((diffs,),np.mean,confidence_level=confidence)\n",
    "    \n",
    "    return np.mean(diffs), np.std(diffs),res.confidence_interval.low, res.confidence_interval.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f359819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_diff(ts_exp, ts_crtl, confidence=0.95):\n",
    "    pooled_var = calculate_pooled_variance(ts_crtl, ts_exp)\n",
    "    std_error =  np.sqrt((pooled_var)/len(ts_exp)+(pooled_var)/len(ts_crtl))\n",
    "    diff = ts_exp.mean()-ts_crtl.mean()\n",
    "    t_val = abs(diff/std_error)\n",
    "    t_crit = st.t.ppf(q=1-confidence/2, df=len(ts_exp)+ len(ts_crtl)-2)\n",
    "    \n",
    "    return t_val.values, std_error,np.sqrt(pooled_var),t_crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f021567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(snakemake.input.data_tracker, 'r') as f:\n",
    "    data_vars = yaml.load(f, yaml.SafeLoader)\n",
    "    \n",
    "erfs_vars = data_vars['ERFs']\n",
    "cat = intake.open_esm_datastore(snakemake.input.catalog, registry=dvr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f5ef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_forcing_table(erfs_list, cat,nSamples=1000,confidence_level=0.95, mask=None, only_regrid=False):\n",
    "    erfs = {k : v for k,v in conf['variables'].items() if k in erfs_list}\n",
    "    \n",
    "    forcings = {k : v for k, v in erfs.items() if k.startswith('ERF')}\n",
    "\n",
    "    derived_forcings = {k : v for k, v in erfs.items() if k.startswith('ERF') == False}\n",
    "    df = pd.DataFrame(index=erfs_list, \n",
    "                      columns=['diff', 'diff_std', 'diff_ci_low', 'diff_ci_high','mean_exp',\n",
    "                               'mean_ctrl', 'std_exp', 'std_ctrl', 't_val', 'diff_sigificant', 'std_error'])\n",
    "    \n",
    "    req_derived_ERFs = []\n",
    "    for k, reqERF in derived_forcings.items():\n",
    "        for e in reqERF:\n",
    "            if e not in req_derived_ERFs:\n",
    "                req_derived_ERFs.append(e)\n",
    "    additional_ERFs = {k : conf['variables'][k] for k in req_derived_ERFs if k not in forcings.keys()}\n",
    "    forcings_ts_exp = {}\n",
    "    forcings_ts_ctrl = {}\n",
    "    forcings = {**forcings, **additional_ERFs}\n",
    "    for k,listvar in forcings.items():   \n",
    "        exp_imbalance = calc_erf(listvar,exp_id,cat,time_slice=time_slice, n_samples=nSamples, mask=mask,\n",
    "                                only_regrid=only_regrid)\n",
    "        forcings_ts_exp[k] = exp_imbalance[-1] \n",
    "        df.loc[k, 'mean_exp'] = exp_imbalance[0]; df.loc[k, 'std_exp'] = exp_imbalance[1]\n",
    "        ctrl_imbalance = calc_erf(listvar,ctrl_id,cat,time_slice=time_slice, n_samples=nSamples,mask=mask,\n",
    "                                 only_regrid=only_regrid)\n",
    "        forcings_ts_ctrl[k] = ctrl_imbalance[-1] \n",
    "        t_val, st_error,pooled_std,t_crit = t_test_diff(exp_imbalance[-1] ,ctrl_imbalance[-1],confidence_level)\n",
    "        df.loc[k, 'mean_ctrl'] = ctrl_imbalance[0]; df.loc[k, 'std_ctrl'] = ctrl_imbalance[1]\n",
    "        diff_m,diff_std, ciL, ciH = bootstrap_diff(exp_imbalance[0],exp_imbalance[1],ctrl_imbalance[0], \n",
    "                                          ctrl_imbalance[1], nSamples,confidence_level)\n",
    "        df.loc[k,'diff'] = diff_m; df.loc[k,'diff_ci_low'] = ciL; df.loc[k,'diff_ci_high'] = ciH\n",
    "        df.loc[k, 'diff_std'] = diff_std\n",
    "        df.loc[k,'t_val'] = t_val\n",
    "        df.loc[k, 'diff_sigificant'] = t_val > t_crit\n",
    "        df.loc[k,'st_error'] = st_error\n",
    "        df.loc[k,'pooled_std'] = pooled_std\n",
    "    for k, listvar in derived_forcings.items():\n",
    "        erf_tot = df.loc[listvar[0],'diff']\n",
    "        erf_tot_std = df.loc[listvar[0],'diff_std']\n",
    "        erf_effect = df.loc[listvar[1],'diff']\n",
    "        erf_effect_std = df.loc[listvar[1],'diff_std']\n",
    "        diff_m,diff_std, ciL, ciH = bootstrap_diff(erf_tot, erf_tot_std, erf_effect, \n",
    "                                                    erf_effect_std, nSamples, confidence_level)\n",
    "        df.loc[k,'diff'] = diff_m; df.loc[k,'diff_ci_low'] = ciL; df.loc[k,'diff_ci_high'] = ciH\n",
    "        df.loc[k, 'diff_std'] = diff_std\n",
    "        df.loc[k, 'mean_exp'] = df.loc[listvar[0],'mean_exp'] - df.loc[listvar[1],'mean_exp']\n",
    "        df.loc[k, 'mean_ctrl'] = df.loc[listvar[0],'mean_ctrl'] - df.loc[listvar[1],'mean_ctrl']\n",
    "        df.loc[k, 'std_ctrl'] = (forcings_ts_ctrl[listvar[0]] - forcings_ts_ctrl[listvar[1]]).std().values\n",
    "        df.loc[k, 'std_exp'] = (forcings_ts_exp[listvar[0]] - forcings_ts_exp[listvar[1]]).std().values\n",
    "        t_val,st_error,pooled_std, t_crit = t_test_diff(forcings_ts_exp[listvar[0]] - forcings_ts_exp[listvar[1]], \n",
    "                                   forcings_ts_ctrl[listvar[0]] - forcings_ts_ctrl[listvar[1]], confidence_level)\n",
    "        df.loc[k,'t_val'] = t_val\n",
    "        df.loc[k,'st_error'] = st_error\n",
    "        df.loc[k, 'diff_sigificant'] = t_val > t_crit\n",
    "        df.loc[k,'pooled_std'] = pooled_std\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "if snakemake.rule == \"calc_dust_regional_erf_table\":\n",
    "    import os\n",
    "    if os.path.exists(snakemake.output.outpath_all) == False:\n",
    "        df_all = setup_forcing_table(erfs_vars, cat, nSamples, confidence_level, only_regrid=True)\n",
    "        df_all.to_csv(snakemake.output.outpath_all)\n",
    "    \n",
    "    if os.path.exists(snakemake.output.outpath_unmasked) == False:\n",
    "        df_inversemask = setup_forcing_table(erfs_vars, cat, nSamples, confidence_level, mask=MASK[mod_id]<1) \n",
    "        df_inversemask.to_csv(snakemake.output.outpath_unmasked)\n",
    "    if os.path.exists(snakemake.output.outpath_masked) == False:\n",
    "        df_masked = setup_forcing_table(erfs_vars, cat, nSamples, confidence_level, mask=MASK[mod_id]>0)\n",
    "        df_masked.to_csv(snakemake.output.outpath_masked)\n",
    "    \n",
    "\n",
    "else:\n",
    "    df = setup_forcing_table(erfs_vars, cat, nSamples, confidence_level)\n",
    "    df.to_csv(snakemake.output.outpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
